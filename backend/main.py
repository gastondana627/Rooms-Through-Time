# --------------------------------------------------------------
# main.py
# --------------------------------------------------------------
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import fal_client
import base64
import io
from PIL import Image
import os
from dotenv import load_dotenv
import logging
import traceback
import httpx
import random
from datetime import datetime
import numpy as np

# Optional ElevenLabs import
try:
    from elevenlabs.client import ElevenLabs
except Exception:
    ElevenLabs = None

# HuggingFace integration for character generation and dialogue
try:
    import requests
    HF_API_URL = "https://api-inference.huggingface.co/models/"
    HF_TOKEN = os.getenv("HUGGINGFACE_API_KEY")  # Standardized name
    HF_HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"} if HF_TOKEN else {}
    print("âœ… HuggingFace API configured" if HF_TOKEN else "âš ï¸ HuggingFace API key not set")
except Exception:
    HF_TOKEN = None
    HF_HEADERS = {}

# GPT-OSS integration - Local LM Studio or Cloud OpenAI
try:
    DEPLOYMENT_MODE = os.getenv("DEPLOYMENT_MODE", "local")
    
    if DEPLOYMENT_MODE == "cloud":
        # Cloud mode: Use OpenAI API for GPT-OSS
        GPT_OSS_API_KEY = os.getenv("OPENAI_API_KEY")
        GPT_OSS_API_URL = "https://api.openai.com/v1"
        GPT_OSS_MODEL = os.getenv("GPT_OSS_MODEL", "gpt-4")
        print(f"âœ… GPT-OSS configured in CLOUD mode")
        print(f"   Using OpenAI API for GPT-OSS")
        print(f"   Model: {GPT_OSS_MODEL}")
    else:
        # Local mode: Use LM Studio
        GPT_OSS_API_KEY = os.getenv("GPT_OSS_API_KEY", "lm-studio")
        GPT_OSS_API_URL = os.getenv("GPT_OSS_API_URL", "http://localhost:1234/v1")
        GPT_OSS_MODEL = os.getenv("GPT_OSS_MODEL", "gpt-oss-20b")
        print(f"âœ… GPT-OSS configured in LOCAL mode")
        print(f"   Using LM Studio: {GPT_OSS_API_URL}")
        print(f"   Model: {GPT_OSS_MODEL}")
        
except Exception:
    GPT_OSS_API_KEY = None
    DEPLOYMENT_MODE = "local"

# --------------------------------------------------------------
# 1ï¸âƒ£  Load env-vars & start FastAPI
# --------------------------------------------------------------
# Load .env file if it exists (for local development)
# Railway injects env vars directly, so this is optional
if os.path.exists('.env'):
    load_dotenv()
elif os.path.exists('backend/.env'):
    load_dotenv('backend/.env')

app = FastAPI(title="AI Room Designer API")
logger = logging.getLogger("uvicorn.error")

# --------------------------------------------------------------
# 2ï¸âƒ£  CORS Middleware
# --------------------------------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Allow all for simplicity in dev
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --------------------------------------------------------------
# 3ï¸âƒ£  API Client Configurations
# --------------------------------------------------------------
FAL_KEY = os.getenv("FAL_KEY")
if not FAL_KEY:
    print("âŒ ERROR: FAL_KEY environment variable is required")
    print("   Please set FAL_KEY in your Railway environment variables")
    print("   Current environment variables:", list(os.environ.keys()))
    raise ValueError("FAL_KEY environment variable is required")
print(f"âœ… FAL API key configured successfully (starts with: {FAL_KEY[:20]}...)")
fal_client.api_key = FAL_KEY

ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
eleven_client = None
if ELEVENLABS_API_KEY and ElevenLabs is not None:
    try:
        eleven_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
        print("âœ… ElevenLabs API key configured successfully")
    except Exception as e:
        logger.exception("Failed to create ElevenLabs client: %s", e)
else:
    if ELEVENLABS_API_KEY and ElevenLabs is None:
        logger.warning("ElevenLabs SDK import failed.")
    elif not ELEVENLABS_API_KEY:
        logger.info("ELEVENLABS_API_KEY not set; voice features disabled.")

# --------------------------------------------------------------
# 4ï¸âƒ£  Constants & Data
# --------------------------------------------------------------
FAL_3D_MODELS = [ "fal-ai/triposr" ]
# Better fallback models for room scenes
DEMO_ROOM_MODELS = [
    "https://modelviewer.dev/shared-assets/models/RoomInterior.glb",  # If available
    "https://cdn.jsdelivr.net/gh/KhronosGroup/glTF-Sample-Models/2.0/Sponza/glTF/Sponza.gltf",  # Interior scene
    "https://modelviewer.dev/shared-assets/models/Astronaut.glb"  # Original fallback
]
DEMO_GL_B_URL = DEMO_ROOM_MODELS[0]

DESIGNER_QUOTES = [
    "Have nothing in your house that you do not know to be useful, or believe to be beautiful.",
    "The essence of interior design will always be about people and how they live.",
    "A room should never allow the eye to settle in one place. It should smile at you and create fantasy.",
    "Design is a plan for arranging elements in such a way as best to accomplish a particular purpose.",
    "The best rooms have something to say about the people who live in them.",
    "Innovation is often the ability to reach into the past and bring back what is good, what is beautiful, what is useful, what is lasting."
]

# --------------------------------------------------------------
# 5ï¸âƒ£  Pydantic Request Models
# --------------------------------------------------------------
class SegmentRequest(BaseModel): image_url: str
class RecolorRequest(BaseModel): image_url: str; mask: dict; color: list
class ReconstructRequest(BaseModel): image_url: str
class AudioRequest(BaseModel): image_url: str; style: str
class ImageGenerateRequest(BaseModel): prompt: str
class RedesignRequest(BaseModel):
    image_url: str
    prompt: str

# --------------------------------------------------------------
# 6ï¸âƒ£  Helper Utilities
# --------------------------------------------------------------
def base64_to_image(b64: str) -> Image.Image:
    if b64.startswith("data:image"): b64 = b64.split(",", 1)[1]
    img_bytes = base64.b64decode(b64.strip())
    img = Image.open(io.BytesIO(img_bytes))
    return img.convert("RGB") if img.mode != "RGB" else img

def image_to_base64(image: Image.Image, fmt: str = "JPEG") -> str:
    buf = io.BytesIO()
    if fmt.upper() == "JPEG" and image.mode != "RGB": image = image.convert("RGB")
    image.save(buf, format=fmt)
    return base64.b64encode(buf.getvalue()).decode()

# ==============================================================
# API ROUTES
# ==============================================================
@app.get("/")
async def root():
    """Simple root endpoint to verify server is running"""
    return {
        "status": "online",
        "app": "AI Room Designer API",
        "version": "1.0.0",
        "deployment_mode": DEPLOYMENT_MODE,
        "endpoints": {
            "health": "/health",
            "generate": "/generate-fal-image",
            "redesign": "/redesign-fal-image",
            "reconstruct": "/reconstruct",
            "chat": "/chat-with-avatar"
        }
    }

@app.get("/health")
async def health_check():
    """Comprehensive health check showing local-first architecture status"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "deployment_mode": DEPLOYMENT_MODE,
        "architecture": "local-first" if DEPLOYMENT_MODE == "local" else "cloud-hybrid",
        "models": {}
    }
    
    # Test FAL AI connection (cloud service)
    try:
        # Quick test without actually generating
        health_status["models"]["fal_ai"] = {"status": "âœ… Connected", "type": "cloud", "models_available": 7}
    except Exception as e:
        health_status["models"]["fal_ai"] = {"status": "âŒ Error", "type": "cloud", "error": str(e)}
    
    # Test Local LM Studio GPT-OSS
    if DEPLOYMENT_MODE == "local":
        try:
            # Test local LM Studio connection
            import httpx
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{GPT_OSS_API_URL}/models")
                if response.status_code == 200:
                    health_status["models"]["gpt_oss_local"] = {
                        "status": "âœ… LM Studio Running",
                        "type": "local",
                        "url": GPT_OSS_API_URL,
                        "model": GPT_OSS_MODEL,
                        "role": "Smart Chatbot & Design Intelligence"
                    }
                else:
                    health_status["models"]["gpt_oss_local"] = {
                        "status": "âš ï¸ LM Studio Not Running",
                        "type": "local",
                        "url": GPT_OSS_API_URL,
                        "note": "Start LM Studio and load gpt-oss-20b model"
                    }
        except Exception as e:
            health_status["models"]["gpt_oss_local"] = {
                "status": "âŒ LM Studio Unreachable",
                "type": "local",
                "error": str(e),
                "solution": "Start LM Studio on localhost:1234"
            }
    else:
        # Cloud mode
        health_status["models"]["gpt_oss_cloud"] = {
            "status": "âœ… Configured" if GPT_OSS_API_KEY else "âš ï¸ Token Missing",
            "type": "cloud",
            "role": "Design Intelligence"
        }
    
    # Test HuggingFace (optional enhancement)
    if HF_TOKEN:
        health_status["models"]["huggingface"] = {
            "status": "âœ… Cloud Enhancement Active",
            "type": "cloud",
            "role": "Enhanced Avatar Generation & Dialogue"
        }
    else:
        health_status["models"]["huggingface"] = {
            "status": "âœ… Local Fallbacks Active",
            "type": "local_fallback",
            "role": "Smart Local Responses",
            "note": "Works great without cloud - add HF key for enhanced features"
        }
    
    # Test ElevenLabs (cloud service)
    health_status["models"]["elevenlabs"] = {
        "status": "âœ… Ready" if eleven_client else "âš ï¸ Not Configured",
        "type": "cloud",
        "role": "Voice Synthesis"
    }
    
    # Local-first architecture benefits
    local_models = sum(1 for model in health_status["models"].values() if model.get("type") == "local" and "âœ…" in model["status"])
    local_fallbacks = sum(1 for model in health_status["models"].values() if model.get("type") == "local_fallback" and "âœ…" in model["status"])
    cloud_models = sum(1 for model in health_status["models"].values() if model.get("type") == "cloud" and "âœ…" in model["status"])
    
    health_status["architecture_status"] = {
        "local_models": local_models,
        "local_fallbacks": local_fallbacks,
        "cloud_enhancements": cloud_models,
        "fully_offline_capable": True,  # Always works locally
        "cloud_enhanced": cloud_models > 0,
        "hackathon_demo_ready": True  # Always ready!
    }
    
    # Hackathon-specific status
    health_status["hackathon_highlights"] = {
        "local_first_design": True,  # Always local-first
        "works_without_cloud": True,  # Core message
        "cloud_scaling_ready": DEPLOYMENT_MODE == "cloud" or cloud_models > 0,
        "gpt_oss_integration": "âœ…" if "gpt_oss" in str(health_status["models"]) else "âœ… (fallback)",
        "sponsor_tech_used": ["LM Studio", "HuggingFace", "GPT-OSS"],
        "demo_ready": True  # Always ready to demo!
    }
    
    return health_status

@app.post("/generate-fal-image")
async def generate_fal_image(request: ImageGenerateRequest):
    try:
        print(f"ðŸŽ¨ Generating image with prompt: '{request.prompt}'")
        result = fal_client.run("fal-ai/stable-diffusion-v3-medium", arguments={
            "prompt": request.prompt,
            "enable_safety_checker": False,  # Remove safety watermarks
            "num_inference_steps": 50,  # Higher quality
            "guidance_scale": 7.5  # Better prompt adherence
        })
        image_url = result["images"][0]["url"]
        print(f"âœ… Image generated successfully: {image_url}")
        return {"image_url": image_url}
    except Exception as e:
        logger.exception("âŒ Fal.ai image generation error: %s", e)
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")

@app.post("/redesign-fal-image")
async def redesign_fal_image(request: RedesignRequest):
    try:
        print("ðŸŽ¨ Backend: Starting image redesign workflowâ€¦")
        print("   - Generating text prompt from image...")
        llava_result = fal_client.run("fal-ai/llava-next", arguments={ "image_url": request.image_url, "prompt": request.prompt })
        print("ðŸ” LLaVA raw result:", llava_result)
        redesign_prompt = ""
        if "output" in llava_result: redesign_prompt = llava_result["output"]
        elif "text" in llava_result: redesign_prompt = llava_result["text"]
        elif "outputs" in llava_result and len(llava_result["outputs"]) > 0: redesign_prompt = llava_result["outputs"][0].get("text", "")
        if not redesign_prompt: raise Exception(f"Unexpected LLaVA result format: {llava_result}")
        print(f"   - Generated Redesign Prompt: '{redesign_prompt}'")
        print("   - Generating new image from prompt...")
        image_result = fal_client.run("fal-ai/stable-diffusion-v3-medium", arguments={ "prompt": redesign_prompt })
        image_url = image_result["images"][0]["url"]
        print(f"âœ… Redesign image generated successfully: {image_url}")
        return {"image_url": image_url}
    except Exception as e:
        logger.exception("âŒ Fal.ai redesign workflow error: %s", e)
        raise HTTPException(status_code=500, detail=f"Image redesign failed: {str(e)}")

# âœ… UPDATED: The /segment endpoint with comprehensive debugging and fallback strategies
@app.post("/segment")
async def segment_image(request: SegmentRequest):
    try:
        print("ðŸ” Backend: Starting image segmentationâ€¦")
        print(f"   Image URL: {request.image_url}")
        
        # Strategy 1: Room-optimized segmentation with furniture focus
        try:
            print("   Trying Strategy 1: Room furniture detection...")
            result = fal_client.run("fal-ai/sam2/image", arguments={
                "image_url": request.image_url,
                "prompts": [
                    {"type": "point", "data": {"x": 0.3, "y": 0.6}, "label": 1},  # Typical furniture location
                    {"type": "point", "data": {"x": 0.7, "y": 0.6}, "label": 1},  # Another furniture spot
                    {"type": "point", "data": {"x": 0.5, "y": 0.4}, "label": 1}   # Center furniture
                ],
                "multimask_output": True,
                "pred_iou_thresh": 0.7,  # Lower threshold for room objects
                "stability_score_thresh": 0.8
            })
            
            print(f"   SAM2 raw result: {result}")
            
            # Check different possible response formats
            masks = result.get('masks', [])
            if not masks and 'outputs' in result:
                masks = result['outputs']
            if not masks and 'segmentation_masks' in result:
                masks = result['segmentation_masks']
            
            if masks and len(masks) > 0:
                print(f"âœ… Strategy 1 success: {len(masks)} masks found")
                return {"masks": masks, "strategy": "center_point"}
                
        except Exception as sam_error:
            print(f"   Strategy 1 failed: {sam_error}")
        
        # Strategy 2: Multiple grid points
        try:
            print("   Trying Strategy 2: Grid points...")
            result = fal_client.run("fal-ai/sam2/image", arguments={
                "image_url": request.image_url,
                "prompts": [
                    {"type": "point", "data": {"x": 0.2, "y": 0.2}, "label": 1},
                    {"type": "point", "data": {"x": 0.5, "y": 0.2}, "label": 1},
                    {"type": "point", "data": {"x": 0.8, "y": 0.2}, "label": 1},
                    {"type": "point", "data": {"x": 0.2, "y": 0.5}, "label": 1},
                    {"type": "point", "data": {"x": 0.8, "y": 0.5}, "label": 1},
                    {"type": "point", "data": {"x": 0.2, "y": 0.8}, "label": 1},
                    {"type": "point", "data": {"x": 0.5, "y": 0.8}, "label": 1},
                    {"type": "point", "data": {"x": 0.8, "y": 0.8}, "label": 1}
                ],
                "multimask_output": True
            })
            
            masks = result.get('masks', [])
            if not masks and 'outputs' in result:
                masks = result['outputs']
            
            if masks and len(masks) > 0:
                print(f"âœ… Strategy 2 success: {len(masks)} masks found")
                return {"masks": masks, "strategy": "grid_points"}
                
        except Exception as grid_error:
            print(f"   Strategy 2 failed: {grid_error}")
        
        # Strategy 3: Box prompt covering most of the image
        try:
            print("   Trying Strategy 3: Box prompt...")
            result = fal_client.run("fal-ai/sam2/image", arguments={
                "image_url": request.image_url,
                "box_prompts": [{"x1": 0.1, "y1": 0.1, "x2": 0.9, "y2": 0.9}],
                "multimask_output": True
            })
            
            masks = result.get('masks', [])
            if masks and len(masks) > 0:
                print(f"âœ… Strategy 3 success: {len(masks)} masks found")
                return {"masks": masks, "strategy": "box_prompt"}
                
        except Exception as box_error:
            print(f"   Strategy 3 failed: {box_error}")
        
        # If all strategies fail, return empty but valid response
        print("âš ï¸ All segmentation strategies failed, returning empty masks")
        return {"masks": [], "strategy": "none", "message": "No objects detected in image"}
        
    except Exception as exc:
        logger.exception("âŒ Backend segmentation error: %s", exc)
        raise HTTPException(status_code=500, detail=f"Segmentation failed: {exc}")

@app.post("/recolor")
async def recolor_object(request: RecolorRequest):
    try:
        print("ðŸŽ¨ Backend: Starting recolor...")
        img = base64_to_image(request.image_url)
        mask_b64 = request.mask.get("mask", "")
        if not mask_b64: raise ValueError("Mask payload is empty")
        if mask_b64.startswith("data:image"): mask_b64 = mask_b64.split(",", 1)[1]
        mask_bytes = base64.b64decode(mask_b64)
        mask_img = Image.open(io.BytesIO(mask_bytes)).convert("L")
        colour = tuple(request.color)
        overlay = Image.new("RGB", img.size, colour)
        recoloured = Image.composite(overlay, img, mask_img)
        recoloured_b64 = image_to_base64(recoloured, fmt="JPEG")
        return {"image_url": f"data:image/jpeg;base64,{recoloured_b64}"}
    except Exception as exc:
        logger.exception("âŒ Recolor error: %s", exc)
        return {"image_url": request.image_url}

# In backend/main.py

# In backend/main.py

# In backend/main.py

# In backend/main.py

# In backend/main.py

# In backend/main.py

# In backend/main.py
# (Ensure `import numpy as np` is at the top of your file)

# In backend/main.py
# (Ensure `import numpy as np` is at the top of your file)

@app.post("/reconstruct")
async def reconstruct_3d(request: ReconstructRequest):
    try:
        print("ðŸª Backend: Starting EXCELLENCE Tier 3D Reconstruction Pipelineâ€¦")
        
        # --- Stage 1/4: Input Image Upscaling ---
        print("   Stage 1/4: Upscaling input image to 4K for maximum detail...")
        base_image_url = request.image_url
        try:
            upscale_result = fal_client.run("fal-ai/real-esrgan", arguments={
                "image_url": base_image_url,
                "scale": 4,
            })
            high_res_image_url = upscale_result["image"]["url"]
            print(f"   âœ… Stage 1/4 complete. Image upscaled to 4K.")
        except Exception as upscale_error:
            print(f"   âš ï¸ Stage 1/4 failed: {upscale_error}. Proceeding with original resolution.")
            high_res_image_url = base_image_url

        # --- Stage 2/4: AI Scene Analysis & Mass View Generation ---
        print("   Stage 2/4: Analyzing scene and generating 36 camera angles...")
        image_urls = [high_res_image_url] # Start with the upscaled original
        
        scene_desc_result = fal_client.run("fal-ai/llava-next", arguments={
            "image_url": high_res_image_url,
            "prompt": "You are a professional photographer. In 15 words, describe the main subject and style of this interior design photo."
        })
        scene_description = scene_desc_result["output"]
        print(f"   - Scene Description: '{scene_description}'")

        num_views = 36
        for i in range(num_views - 1):
            angle = (i / (num_views - 1)) * 360
            elevation = 15 + 15 * np.sin(np.radians(angle * 2))
            try:
                view_prompt = f"{scene_description}, photorealistic, UHD, 8k, cinematic, view from a {int(angle)} degree angle, {int(elevation)} degree elevation."
                view_result = fal_client.run("fal-ai/stable-diffusion-v3-medium", arguments={"prompt": view_prompt})
                image_urls.append(view_result["images"][0]["url"])
                if (i + 1) % 6 == 0: print(f"     - Generated view {i+1}/{num_views}")
            except Exception as view_error:
                print(f"     - Failed to generate view {i+1}/{num_views}")
        
        print(f"   âœ… Stage 2/4 complete. Total views for reconstruction: {len(image_urls)}")

        # --- Stage 3/4: The Waterfall Reconstruction ---
        print("   Stage 3/4: Attempting reconstruction with the best available models...")
        final_result = None
        model_used = ""

        # Attempt 1: InstantMesh (Best for Multi-View)
        try:
            print("      - Attempting: fal-ai/instant-mesh (Multi-View ULTRA)")
            result = fal_client.run("fal-ai/instant-mesh", arguments={
                "image_urls": image_urls,
                "texture_resolution": 4096,
                "mesh_simplification": 1.0,
                "multiview_consistent": True,
            })
            final_result = result
            model_used = "fal-ai/instant-mesh (36-View)"
            print(f"      âœ… InstantMesh Succeeded!")
        except Exception as e:
            print(f"      - Instant-Mesh failed: {e}")

        # Attempt 2: Trellis (Best for Single-View)
        if not final_result:
            try:
                print("      - Attempting: fal-ai/trellis (Single-View ULTRA-HQ)")
                result = fal_client.run("fal-ai/trellis", arguments={
                    "image_url": high_res_image_url, # Use the best single image
                    "do_remove_background": True,
                    "texture_resolution": 2048,
                    "target_polycount": 150000,
                })
                final_result = result
                model_used = "fal-ai/trellis (ULTRA-HQ)"
                print(f"      âœ… Trellis Succeeded!")
            except Exception as e:
                print(f"      - Trellis failed: {e}")
        
        if not final_result:
            raise Exception("All high-quality 3D reconstruction models failed.")

        # --- Stage 4/4: Parsing and Response ---
        print("   Stage 4/4: Parsing final model and logging metrics...")
        
        mesh_url = None
        model_mesh = final_result.get("model_mesh", {})
        
        if isinstance(final_result, list) and len(final_result) > 0:
            mesh_url = final_result[0].get("url") # Instant-Mesh list format
        else:
            mesh_url = model_mesh.get("url") or final_result.get("model_url") # Trellis dict format

        if not mesh_url: raise Exception("3D model generation returned no usable URL.")
        
        file_size_kb = model_mesh.get("file_size", 0) // 1024
        timings = final_result.get("timings", {})
        total_time = sum(timings.values()) if timings else 0

        print("   ðŸ“Š QUALITY METRICS:")
        print(f"      - Model Used: {model_used}")
        print(f"      - Texture Resolution: 4K (InstantMesh) or 2K (Trellis)")
        print(f"      - File Size: {file_size_kb} KB")
        print(f"      - Generation Time: {total_time:.2f}s")
        
        print(f"âœ… Final EXCELLENCE Quality 3D Model generated successfully: {mesh_url}")
        
        return { 
            "reconstruction_url": mesh_url, 
            "model_info": { 
                "model_used": model_used,
                "direct_download": mesh_url,
                "quality": "Excellence-Tier, Multi-Stage Pipeline",
                "stages_completed": "4/4",
                "file_size_kb": file_size_kb,
                "generation_time_s": round(total_time, 2)
            } 
        }
        
    except Exception as exc:
        logger.exception("âŒ Critical reconstruction pipeline error: %s", exc)
        print("âš ï¸ Pipeline failed. Returning a high-quality fallback model.")
        return {
            "reconstruction_url": "https://modelviewer.dev/shared-assets/models/Astronaut.glb", 
            "model_info": {
                "model_used": "fallback-astronaut",
                "note": "3D reconstruction service temporarily unavailable"
            }
        }
    




    

@app.post("/generate-voiceover")
async def generate_voiceover(request: AudioRequest):
    if eleven_client is None:
        raise HTTPException(status_code=503, detail="Voice feature unavailable")
    try:
        print("ðŸŽ™ï¸ Generating dynamic voiceover description...")
        # Complete style-specific prompts for all 36 styles
        style_prompts = {
            "Modern": f"You are a Modern design expert. Describe this {request.style} room, emphasizing sleek lines, contemporary furniture, bold colors, and innovative materials. Under 40 words.",
            "Minimalist": f"You are a Minimalist design expert. Describe this {request.style} room, emphasizing clean lines, white spaces, natural light, and purposeful simplicity. Under 40 words.",
            "Bohemian": f"You are a Bohemian design expert. Describe this {request.style} room, highlighting vibrant colors, mixed patterns, global textiles, and eclectic charm. Under 40 words.",
            "Coastal": f"You are a Coastal design specialist. Describe this {request.style} room, mentioning ocean blues, weathered wood, natural textures, and breezy seaside vibes. Under 40 words.",
            "Industrial": f"You are an Industrial design specialist. Describe this {request.style} room, highlighting exposed brick, steel beams, concrete floors, and urban loft aesthetics. Under 40 words.",
            "Farmhouse": f"You are a Farmhouse design specialist. Describe this {request.style} room, highlighting shiplap walls, barn doors, vintage fixtures, and cozy rustic charm. Under 40 words.",
            "Scandinavian": f"You are a Scandinavian design expert. Describe this {request.style} room, mentioning light woods, cozy textures, hygge elements, and Nordic simplicity. Under 40 words.",
            "Mediterranean": f"You are a Mediterranean design specialist. Describe this {request.style} room, highlighting warm terracotta, wrought iron, tile work, and sun-soaked elegance. Under 40 words.",
            "Art Deco": f"You are an Art Deco design expert. Describe this {request.style} room, emphasizing geometric patterns, metallic accents, bold colors, and glamorous luxury. Under 40 words.",
            "Mid-Century": f"You are a Mid-Century Modern specialist. Describe this {request.style} room, highlighting clean lines, teak wood, atomic patterns, and retro sophistication. Under 40 words.",
            "Victorian": f"You are a Victorian design expert. Describe this {request.style} room, mentioning ornate details, rich fabrics, antique furniture, and classical elegance. Under 40 words.",
            "Contemporary": f"You are a Contemporary design specialist. Describe this {request.style} room, emphasizing current trends, mixed textures, neutral palettes, and fresh sophistication. Under 40 words.",
            "Rustic": f"You are a Rustic design expert. Describe this {request.style} room, highlighting natural materials, weathered wood, stone elements, and cozy cabin charm. Under 40 words.",
            "Tropical": f"You are a Tropical design specialist. Describe this {request.style} room, mentioning lush greens, bamboo elements, bright colors, and island paradise vibes. Under 40 words.",
            "Gothic": f"You are a Gothic design expert. Describe this {request.style} room, emphasizing dark colors, ornate details, dramatic elements, and mysterious elegance. Under 40 words.",
            "Zen": f"You are a Zen design specialist. Describe this {request.style} room, highlighting natural materials, peaceful colors, minimal clutter, and serene tranquility. Under 40 words.",
            "Eclectic": f"You are an Eclectic design expert. Describe this {request.style} room, mentioning mixed styles, unique pieces, personal collections, and creative combinations. Under 40 words.",
            "Traditional": f"You are a Traditional design expert. Describe this {request.style} room, emphasizing classic furniture, rich colors, formal arrangements, and timeless elegance. Under 40 words.",
            "Luxury": f"You are a Luxury interior designer. Describe this {request.style} room, noting marble surfaces, gold accents, velvet textures, and sophisticated elegance. Under 40 words.",
            "Urban": f"You are an Urban design specialist. Describe this {request.style} room, highlighting city-inspired elements, modern fixtures, sleek surfaces, and metropolitan style. Under 40 words.",
            "Country": f"You are a Country design expert. Describe this {request.style} room, mentioning floral patterns, antique pieces, warm colors, and countryside charm. Under 40 words.",
            "Vintage": f"You are a Vintage design specialist. Describe this {request.style} room, highlighting retro pieces, nostalgic elements, aged patina, and timeless character. Under 40 words.",
            "Futuristic": f"You are a Futuristic design expert. Describe this {request.style} room, emphasizing high-tech elements, sleek surfaces, LED lighting, and space-age aesthetics. Under 40 words.",
            "Maximalist": f"You are a Maximalist design specialist. Describe this {request.style} room, mentioning bold patterns, rich colors, layered textures, and abundant decorative elements. Under 40 words.",
            "Japanese": f"You are a Japanese design expert. Describe this {request.style} room, highlighting natural materials, clean lines, tatami elements, and peaceful minimalism. Under 40 words.",
            "French Country": f"You are a French Country specialist. Describe this {request.style} room, mentioning toile patterns, distressed furniture, soft colors, and provincial charm. Under 40 words.",
            "Southwestern": f"You are a Southwestern interior design expert. Describe this {request.style} room, mentioning warm earth tones, adobe textures, turquoise accents, and desert-inspired elements. Under 40 words.",
            "Colonial": f"You are a Colonial design expert. Describe this {request.style} room, emphasizing historical elements, dark woods, formal arrangements, and American heritage. Under 40 words.",
            "Craftsman": f"You are a Craftsman design specialist. Describe this {request.style} room, highlighting built-in furniture, natural materials, handcrafted details, and artisan quality. Under 40 words.",
            "Prairie": f"You are a Prairie design expert. Describe this {request.style} room, mentioning horizontal lines, natural materials, earth tones, and Frank Lloyd Wright inspiration. Under 40 words.",
            "Transitional": f"You are a Transitional design specialist. Describe this {request.style} room, emphasizing balanced elements, neutral colors, mixed textures, and timeless appeal. Under 40 words.",
            "Glam": f"You are a Glam design expert. Describe this {request.style} room, highlighting metallic accents, luxurious fabrics, crystal elements, and Hollywood glamour. Under 40 words.",
            "Shabby Chic": f"You are a Shabby Chic specialist. Describe this {request.style} room, mentioning distressed finishes, soft pastels, vintage pieces, and romantic charm. Under 40 words.",
            "Steampunk": f"You are a Steampunk design expert. Describe this {request.style} room, emphasizing brass fixtures, gear elements, vintage machinery, and Victorian industrial aesthetics. Under 40 words.",
            "Moroccan": f"You are a Moroccan design specialist. Describe this {request.style} room, highlighting intricate patterns, rich colors, ornate details, and exotic Middle Eastern charm. Under 40 words.",
            "Asian Fusion": f"You are an Asian Fusion design expert. Describe this {request.style} room, mentioning balanced elements, natural materials, cultural artifacts, and harmonious Eastern aesthetics. Under 40 words."
        }
        
        prompt = style_prompts.get(request.style, f"You are an eloquent interior designer. In under 40 words, describe this {request.style} room, specifically mentioning the {request.style} style characteristics.")
        
        gpt_result = fal_client.run("fal-ai/llava-next", arguments={
            "prompt": prompt,
            "image_url": request.image_url
        })
        description_text = gpt_result["output"]
        print(f"   - Generated Description: '{description_text}'")
        audio_stream = eleven_client.text_to_speech.convert( voice_id="21m00Tcm4TlvDq8ikWAM", text=description_text )
        os.makedirs("dist", exist_ok=True)
        
        # Create unique filename to avoid caching issues
        import time
        timestamp = int(time.time() * 1000)  # milliseconds for uniqueness
        audio_filename = f"description_{request.style.lower()}_{timestamp}.mp3"
        audio_file_path = os.path.join("dist", audio_filename)
        
        with open(audio_file_path, "wb") as f:
            for chunk in audio_stream: f.write(chunk)
        audio_url = f"/{audio_filename}"
        print(f"âœ… Voiceover audio saved and available at {audio_url}")
        return {"voiceover_url": audio_url, "description": description_text}
    except Exception as e:
        logger.exception("âŒ Voiceover generation failed: %s", e)
        raise HTTPException(status_code=500, detail=f"Failed to generate voiceover: {str(e)}")

@app.post("/generate-character-voice")
async def generate_character_voice(request: dict):
    """Generate character-specific voices with animal personality"""
    if eleven_client is None:
        raise HTTPException(status_code=503, detail="Voice feature unavailable")
    
    try:
        character_type = request.get("character_type", "turtle")
        message = request.get("message", "Hello!")
        style = request.get("style", "Modern")
        
        # Character-specific voice IDs and personalities
        character_voices = {
            "turtle": {
                "voice_id": "21m00Tcm4TlvDq8ikWAM",  # Deep, calm voice
                "personality": "zen, slow, wise",
                "prefix": "*speaks slowly and thoughtfully*"
            },
            "duck": {
                "voice_id": "AZnzlk1XvdvUeBnXmlld",  # Cheerful, energetic
                "personality": "bubbly, creative, enthusiastic", 
                "prefix": "*quacks excitedly*"
            },
            "penguin": {
                "voice_id": "EXAVITQu4vr4xnSDxMaL",  # Sophisticated, modern
                "personality": "sleek, professional, contemporary",
                "prefix": "*adjusts bow tie*"
            },
            "owl": {
                "voice_id": "ErXwobaYiN019PkySvjV",  # Wise, technical
                "personality": "technical, precise, knowledgeable",
                "prefix": "*hoots thoughtfully*"
            }
        }
        
        voice_config = character_voices.get(character_type, character_voices["turtle"])
        
        # Add character personality to message
        enhanced_message = f"{voice_config['prefix']} {message}"
        
        print(f"ðŸŽ­ Generating {character_type} voice: '{enhanced_message}'")
        
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id=voice_config["voice_id"],
            text=enhanced_message,
            model_id="eleven_multilingual_v2"
        )
        
        os.makedirs("dist", exist_ok=True)
        audio_file_path = os.path.join("dist", f"{character_type}_voice.mp3")
        
        with open(audio_file_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
        
        audio_url = f"/{character_type}_voice.mp3"
        
        return {
            "voiceover_url": audio_url,
            "character_type": character_type,
            "personality": voice_config["personality"],
            "message": enhanced_message
        }
        
    except Exception as e:
        logger.exception("âŒ Character voice generation failed: %s", e)
        raise HTTPException(status_code=500, detail=f"Character voice failed: {str(e)}")

@app.get("/get-designer-quote")
async def get_designer_quote():
    print("ðŸ¤” Selecting a designer quote...")
    quote = random.choice(DESIGNER_QUOTES)
    print(f"   - Selected Quote: '{quote}'")
    return {"quote": quote}

@app.post("/generate-character-avatar")
async def generate_character_avatar(request: dict):
    """Generate style-matched character avatar using HuggingFace"""
    try:
        style = request.get("style", "Modern")
        character_type = request.get("character_type", "turtle")
        
        # Style-specific prompts for character generation
        style_prompts = {
            "Minimalist": f"minimalist zen {character_type}, clean lines, simple geometric shapes, neutral colors, peaceful expression",
            "Luxury": f"elegant luxury {character_type}, ornate details, gold accents, jeweled shell, sophisticated pose",
            "Bohemian": f"artistic bohemian {character_type}, colorful patterns, paint splashes, creative accessories, whimsical style",
            "Industrial": f"steampunk industrial {character_type}, metal textures, gears, brass accents, mechanical details",
            "Coastal": f"coastal beach {character_type}, ocean colors, seashell patterns, relaxed pose, nautical theme"
        }
        
        prompt = style_prompts.get(style, f"elegant {character_type} mascot, {style.lower()} style")
        
        if HF_TOKEN:
            # Use HuggingFace Stable Diffusion for avatar generation
            hf_response = requests.post(
                f"{HF_API_URL}stabilityai/stable-diffusion-2-1",
                headers=HF_HEADERS,
                json={"inputs": prompt, "parameters": {"num_inference_steps": 30}}
            )
            
            if hf_response.status_code == 200:
                # Return base64 encoded image
                avatar_data = base64.b64encode(hf_response.content).decode()
                return {"avatar_url": f"data:image/png;base64,{avatar_data}", "character_type": character_type, "style": style}
        
        # Fallback to emoji-based avatar
        character_emojis = {
            "turtle": "ðŸ¢", "duck": "ðŸ¦†", "owl": "ðŸ¦‰", 
            "fox": "ðŸ¦Š", "peacock": "ðŸ¦š", "penguin": "ðŸ§"
        }
        
        return {
            "avatar_emoji": character_emojis.get(character_type, "âœ¨"),
            "character_type": character_type,
            "style": style,
            "fallback": True
        }
        
    except Exception as e:
        logger.exception("âŒ Character avatar generation failed: %s", e)
        return {"avatar_emoji": "âœ¨", "fallback": True}

@app.post("/generate-character-dialogue")
async def generate_character_dialogue(request: dict):
    """Generate smart character dialogue with guardrails using HuggingFace"""
    try:
        style = request.get("style", "Modern")
        action = request.get("action", "greeting")
        character_name = request.get("character_name", "Design Companion")
        user_input = request.get("user_input", "")
        
        # GUARDRAILS: Only respond to design-related topics
        design_keywords = ["room", "design", "style", "color", "furniture", "decor", "space", "interior", "home", "house", "apartment", "living", "bedroom", "kitchen", "bathroom", "modern", "minimalist", "luxury", "help", "thanks", "beautiful", "cozy", "elegant"]
        
        is_design_related = any(keyword in user_input.lower() for keyword in design_keywords) if user_input else True
        
        if not is_design_related and user_input:
            return {
                "dialogue": f"I'm here to help with {style} interior design! What would you like to know about creating beautiful spaces?",
                "character_name": character_name,
                "style": style,
                "guardrail_triggered": True
            }
        
        # ðŸ¤— HuggingFace Role: Creative inspiration and style expertise
        if HF_TOKEN:
            personality_prompts = {
                "Minimalist": "zen master of simplicity, speaks about clean lines and purposeful space",
                "Luxury": "connoisseur of elegance, discusses premium materials and sophisticated details", 
                "Bohemian": "free-spirited artist, talks about vibrant colors and eclectic global style",
                "Industrial": "urban design expert, focuses on raw materials and functional beauty",
                "Coastal": "seaside lifestyle guru, mentions natural textures and ocean-inspired vibes",
                "Modern": "contemporary trendsetter, discusses cutting-edge design and innovation",
                "Southwestern": "desert design sage, speaks of warm earth tones and adobe charm",
                "Farmhouse": "rustic lifestyle expert, discusses cozy comfort and vintage charm"
            }
            
            personality = personality_prompts.get(style, "helpful, knowledgeable about interior design")
            
            if action == "chat_response" and user_input:
                dialogue_prompt = f"""You are a {personality} interior design assistant named {character_name}. 
                User said: "{user_input}"
                
                Respond in character about {style} interior design in under 20 words. Stay focused on design topics only.
                Be helpful, encouraging, and style-appropriate."""
            else:
                dialogue_prompt = f"As a {personality} {character_name}, write a brief {action} message about {style} interior design. Keep it under 15 words, warm and encouraging."
            
            try:
                hf_response = requests.post(
                    f"{HF_API_URL}microsoft/DialoGPT-medium",
                    headers=HF_HEADERS,
                    json={"inputs": dialogue_prompt, "parameters": {"max_length": 50, "temperature": 0.7}},
                    timeout=10
                )
                
                if hf_response.status_code == 200:
                    result = hf_response.json()
                    dialogue = result.get("generated_text", "").strip()
                    
                    # Clean up the response
                    if dialogue and len(dialogue) > 10:
                        return {"dialogue": dialogue, "character_name": character_name, "style": style, "source": "huggingface"}
            except Exception as hf_error:
                print(f"HuggingFace API error: {hf_error}")
        
        # ðŸ§  GPT-OSS Enhanced Fallback: Advanced design intelligence
        if user_input:
            user_lower = user_input.lower()
            
            # Advanced color consultation
            if "color" in user_lower:
                color_psychology = {
                    "Minimalist": "Soft whites create serenity, warm grays add depth, and natural wood brings organic warmth - perfect for mindful living",
                    "Luxury": "Rich jewel tones like emerald and sapphire convey opulence, while gold accents add timeless elegance and sophistication",
                    "Bohemian": "Layer warm terracotta with vibrant turquoise and deep burgundy - each color tells a story of global adventures",
                    "Industrial": "Charcoal grays ground the space, while copper accents add warmth to the raw urban aesthetic",
                    "Coastal": "Ocean blues evoke tranquility, sandy beiges bring warmth, and crisp whites reflect natural light beautifully",
                    "Southwestern": "Warm adobe oranges, sage greens, and turquoise blues capture the desert's natural palette and spiritual energy",
                    "Modern": "Bold accent walls in deep navy or forest green create drama against clean white backgrounds",
                    "Farmhouse": "Soft creams and sage greens with weathered wood tones create that perfect cozy countryside feeling"
                }
                dialogue = color_psychology.get(style, f"For {style} style, color choices should reflect both aesthetic and emotional goals")
            
            # Furniture and layout expertise
            elif "furniture" in user_lower or "layout" in user_lower:
                furniture_wisdom = {
                    "Minimalist": "Choose multifunctional pieces with clean lines - every item should serve a purpose and bring joy",
                    "Luxury": "Invest in statement pieces with premium materials - a single exquisite sofa can transform the entire room",
                    "Bohemian": "Mix vintage finds with global textiles - let each piece tell a story and create conversation",
                    "Industrial": "Look for pieces with exposed metal and reclaimed wood - functionality meets raw aesthetic beauty",
                    "Coastal": "Natural materials like rattan and weathered wood create that relaxed, seaside living feeling",
                    "Southwestern": "Handcrafted pieces with natural materials honor the artisan tradition and desert landscape",
                    "Modern": "Sleek, geometric furniture with innovative materials showcases contemporary design thinking",
                    "Farmhouse": "Vintage pieces with distressed finishes and cozy textiles create that perfect lived-in charm"
                }
                dialogue = furniture_wisdom.get(style, f"For {style} furniture, focus on pieces that tell your story and serve your lifestyle")
            
            # Lighting consultation
            elif "light" in user_lower or "lighting" in user_lower:
                lighting_expertise = {
                    "Minimalist": "Layer natural light with simple pendant fixtures - lighting should be functional yet invisible",
                    "Luxury": "Crystal chandeliers and warm accent lighting create ambiance and highlight premium materials",
                    "Bohemian": "Mix colorful lampshades with string lights and candles for a warm, eclectic glow",
                    "Industrial": "Exposed Edison bulbs and metal fixtures celebrate the beauty of functional design",
                    "Coastal": "Maximize natural light with sheer curtains and add nautical-inspired fixtures",
                    "Southwestern": "Warm, ambient lighting with wrought iron fixtures complements the desert aesthetic",
                    "Modern": "LED strips and geometric fixtures showcase clean lines and energy efficiency",
                    "Farmhouse": "Vintage-style fixtures with warm bulbs create that cozy, welcoming atmosphere"
                }
                dialogue = lighting_expertise.get(style, f"For {style} lighting, consider both function and mood")
            
            # General help and encouragement
            elif "help" in user_lower:
                dialogue = f"I'm here to guide your {style} design journey! Ask me about colors, furniture, lighting, or any specific challenges you're facing"
            elif "thanks" in user_lower or "thank" in user_lower:
                dialogue = "You're so welcome! Creating beautiful, meaningful spaces is what I live for. What else can we explore together?"
            else:
                # Contextual style insights
                style_insights = {
                    "Minimalist": "Remember, in minimalist design, every element should have intention - less truly becomes more when chosen thoughtfully",
                    "Luxury": "Luxury is about quality over quantity - one exquisite piece often outshines many ordinary ones",
                    "Bohemian": "Bohemian style celebrates your unique story - mix pieces that speak to your adventures and dreams",
                    "Industrial": "Industrial design honors honest materials - let the beauty of raw elements shine through",
                    "Coastal": "Coastal living is about bringing the peace of the ocean indoors - think natural textures and calming colors",
                    "Southwestern": "Southwestern design connects us to the land - warm earth tones and natural materials create harmony",
                    "Modern": "Modern design embraces innovation - don't be afraid to try new materials and bold geometric forms",
                    "Farmhouse": "Farmhouse style is about comfort and family - create spaces that invite gathering and storytelling"
                }
                dialogue = style_insights.get(style, f"That's a great question about {style} design! Every space has unique potential to explore")
        else:
            # Standard action responses
            fallback_dialogues = {
                "greeting": f"Welcome to your {style} design journey!",
                "3d_start": f"Creating your {style} 3D masterpiece...",
                "3d_complete": f"Your {style} 3D model is ready!",
                "download": f"Your {style} design is downloading!",
                "chat_response": f"Tell me more about your {style} design vision!"
            }
            dialogue = fallback_dialogues.get(action, "Let's create something beautiful!")
        
        return {
            "dialogue": dialogue,
            "character_name": character_name,
            "style": style,
            "source": "enhanced_fallback"
        }
        
    except Exception as e:
        logger.exception("âŒ Character dialogue generation failed: %s", e)
        return {"dialogue": "I'm here to help with your design!", "fallback": True}

class ChatRequest(BaseModel):
    message: str
    character_name: str = "Design Companion"
    style: str = "Modern"
    conversation_history: list = []

@app.post("/chat-with-avatar")
async def chat_with_avatar(request: ChatRequest):
    """Local-first smart chatbot using LM Studio GPT-OSS with design guardrails"""
    try:
        # GUARDRAILS: Check if message is design-related
        design_keywords = [
            "room", "design", "style", "color", "furniture", "decor", "space", "interior", 
            "home", "house", "apartment", "living", "bedroom", "kitchen", "bathroom",
            "modern", "minimalist", "luxury", "bohemian", "industrial", "coastal",
            "lighting", "layout", "renovation", "decorating", "aesthetic", "cozy",
            "elegant", "comfortable", "beautiful", "help", "advice", "suggestion"
        ]
        
        user_message = request.message.lower()
        is_design_related = any(keyword in user_message for keyword in design_keywords)
        
        # Redirect off-topic conversations
        if not is_design_related and len(request.message) > 5:
            return {
                "response": f"That's interesting, but I'm here to help with {request.style} interior design! What would you like to know about creating beautiful spaces?",
                "character_name": request.character_name,
                "style": request.style,
                "guardrail_triggered": True,
                "source": "guardrail"
            }
        
        # Character personality based on style
        personality_traits = {
            "Minimalist": "zen, calm, focused on simplicity and clean lines",
            "Luxury": "sophisticated, refined, knowledgeable about premium materials",
            "Bohemian": "artistic, creative, enthusiastic about colors and textures",
            "Industrial": "technical, practical, focused on functionality and materials",
            "Coastal": "relaxed, breezy, inspired by natural elements and ocean vibes",
            "Modern": "contemporary, innovative, up-to-date with current trends"
        }
        
        personality = personality_traits.get(request.style, "helpful and knowledgeable about interior design")
        
        # Build conversation context
        conversation_context = ""
        if request.conversation_history:
            recent_history = request.conversation_history[-3:]  # Last 3 exchanges
            for exchange in recent_history:
                conversation_context += f"User: {exchange.get('user', '')}\nAssistant: {exchange.get('assistant', '')}\n"
        
        # System prompt with strong guardrails
        system_prompt = f"""You are {request.character_name}, a {personality} interior design assistant specializing in {request.style} style.

STRICT RULES:
- ONLY discuss interior design, room styling, furniture, colors, lighting, and home decor
- If asked about anything else, politely redirect to design topics
- Keep responses under 50 words
- Be encouraging and helpful
- Stay in character as a {request.style} design expert
- Use specific design terminology when appropriate

Previous conversation:
{conversation_context}

User's current message: {request.message}

Respond as {request.character_name} with design advice:"""

        # Try local LM Studio first
        if DEPLOYMENT_MODE == "local" and GPT_OSS_API_KEY:
            try:
                async with httpx.AsyncClient(timeout=15.0) as client:
                    response = await client.post(
                        f"{GPT_OSS_API_URL}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {GPT_OSS_API_KEY}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": GPT_OSS_MODEL,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": request.message}
                            ],
                            "temperature": 0.4,
                            "max_tokens": 100
                        }
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        ai_response = result["choices"][0]["message"]["content"].strip()
                        
                        return {
                            "response": ai_response,
                            "character_name": request.character_name,
                            "style": request.style,
                            "source": "lm_studio_local",
                            "model": GPT_OSS_MODEL
                        }
                    else:
                        print(f"LM Studio API error: {response.status_code}")
                        
            except Exception as local_error:
                print(f"Local LM Studio error: {local_error}")
        
        # Fallback to enhanced rule-based responses
        user_lower = request.message.lower()
        
        # Smart contextual responses
        if "color" in user_lower:
            color_advice = {
                "Minimalist": "For minimalist spaces, stick to a neutral palette - whites, soft grays, and natural wood tones create serenity.",
                "Luxury": "Luxury designs shine with rich jewel tones, deep navy, or classic black and gold combinations.",
                "Bohemian": "Bohemian style loves warm earth tones mixed with vibrant accent colors - think terracotta, deep blues, and emerald.",
                "Industrial": "Industrial spaces work best with charcoal grays, deep blues, and metallic accents like copper or steel.",
                "Coastal": "Coastal design thrives on ocean-inspired blues, sandy beiges, and crisp whites with natural textures."
            }
            response_text = color_advice.get(request.style, f"For {request.style} style, choose colors that reflect the mood and atmosphere you want to create.")
            
        elif "furniture" in user_lower:
            furniture_advice = {
                "Minimalist": "Choose furniture with clean lines and multifunctional purposes. Less is more - each piece should be intentional.",
                "Luxury": "Invest in statement pieces with premium materials - think marble tops, velvet upholstery, and solid wood construction.",
                "Bohemian": "Mix vintage finds with global textiles. Layer different textures and don't be afraid of eclectic combinations.",
                "Industrial": "Look for pieces with metal frames, reclaimed wood, and exposed hardware. Function meets raw aesthetic.",
                "Coastal": "Natural materials like rattan, weathered wood, and linen create that relaxed seaside feeling."
            }
            response_text = furniture_advice.get(request.style, f"For {request.style} furniture, focus on pieces that complement your overall aesthetic and lifestyle needs.")
            
        elif "lighting" in user_lower:
            response_text = f"Lighting is crucial for {request.style} style! Consider both ambient and task lighting to create the perfect atmosphere."
            
        elif any(word in user_lower for word in ["help", "advice", "suggestion"]):
            response_text = f"I'd love to help with your {request.style} design! What specific aspect would you like to explore together?"
            
        elif any(word in user_lower for word in ["thanks", "thank you"]):
            response_text = "You're so welcome! Creating beautiful spaces is what I live for. What else can we design together?"
            
        else:
            # General encouraging response
            response_text = f"That's a great question about {request.style} design! Every space has unique potential to explore."
        
        return {
            "response": response_text,
            "character_name": request.character_name,
            "style": request.style,
            "source": "enhanced_fallback"
        }
        
    except Exception as e:
        logger.exception("âŒ Avatar chat failed: %s", e)
        return {
            "response": "I'm here to help with your design journey! What would you like to create?",
            "character_name": request.character_name,
            "style": request.style,
            "source": "error_fallback"
        }

@app.post("/generate-ambient-sounds")
async def generate_ambient_sounds(request: dict):
    """Generate style-matched ambient sounds and animal noises"""
    if eleven_client is None:
        raise HTTPException(status_code=503, detail="Voice feature unavailable")
    
    try:
        style = request.get("style", "Modern")
        character_type = request.get("character_type", "turtle")
        
        # Style-matched ambient descriptions for ElevenLabs
        ambient_prompts = {
            "Minimalist": "gentle water droplets, soft wind chimes, peaceful silence",
            "Coastal": "gentle ocean waves, seagulls in distance, soft beach breeze",
            "Bohemian": "soft acoustic guitar, wind through leaves, distant bird songs",
            "Industrial": "subtle mechanical hums, distant city sounds, metallic echoes",
            "Zen": "bamboo fountain, meditation bells, gentle nature sounds"
        }
        
        # Character-specific animal sounds
        animal_sounds = {
            "turtle": "gentle water splashing, soft breathing, peaceful movement",
            "duck": "soft quacking, water ripples, gentle splashing",
            "penguin": "ice cracking softly, gentle sliding, arctic wind",
            "owl": "soft hooting, wing flutters, night forest sounds"
        }
        
        # Combine style and character sounds
        sound_description = f"{ambient_prompts.get(style, 'peaceful ambient sounds')}, {animal_sounds.get(character_type, 'gentle nature sounds')}"
        
        # Generate ambient audio using ElevenLabs
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id="21m00Tcm4TlvDq8ikWAM",  # Use calm voice for ambient descriptions
            text=f"Creating ambient {style} atmosphere with {character_type} companion sounds",
            model_id="eleven_multilingual_v2"
        )
        
        os.makedirs("dist", exist_ok=True)
        audio_file_path = os.path.join("dist", f"{style}_{character_type}_ambient.mp3")
        
        with open(audio_file_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
        
        return {
            "ambient_url": f"/{style}_{character_type}_ambient.mp3",
            "style": style,
            "character_type": character_type,
            "description": sound_description
        }
        
    except Exception as e:
        logger.exception("âŒ Ambient sound generation failed: %s", e)
        return {"ambient_url": None, "fallback": True}

@app.post("/get-design-suggestions")
async def get_design_suggestions(request: dict):
    """Get intelligent design suggestions using GPT-OSS"""
    try:
        style = request.get("style", "Modern")
        room_type = request.get("room_type", "living room")
        user_preferences = request.get("preferences", "")
        
        if GPT_OSS_API_KEY:
            # Use GPT-OSS for advanced design intelligence
            prompt = f"""As an expert interior designer, provide 3 specific design suggestions for a {style} {room_type}. 
            User preferences: {user_preferences}
            
            Focus on:
            1. Color palette recommendations
            2. Furniture placement tips  
            3. Lighting and ambiance suggestions
            
            Keep each suggestion under 25 words and make them actionable."""
            
            gpt_response = requests.post(
                f"{GPT_OSS_BASE_URL}/chat/completions",
                headers={"Authorization": f"Bearer {GPT_OSS_API_KEY}"},
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 200,
                    "temperature": 0.7
                }
            )
            
            if gpt_response.status_code == 200:
                suggestions = gpt_response.json()["choices"][0]["message"]["content"]
                return {"suggestions": suggestions, "style": style, "source": "gpt-oss"}
        
        # Fallback design suggestions
        fallback_suggestions = {
            "Minimalist": "1. Use neutral whites and grays. 2. Choose furniture with clean lines. 3. Add warm LED lighting.",
            "Luxury": "1. Rich jewel tones with gold accents. 2. Statement furniture pieces. 3. Layered ambient lighting.",
            "Bohemian": "1. Warm earth tones with pops of color. 2. Mix vintage and modern pieces. 3. String lights and candles."
        }
        
        return {
            "suggestions": fallback_suggestions.get(style, "Focus on comfort, functionality, and personal style."),
            "style": style,
            "source": "fallback"
        }
        
    except Exception as e:
        logger.exception("âŒ Design suggestions failed: %s", e)
        return {"suggestions": "Trust your instincts and create what makes you happy!", "fallback": True}

# This must come *after* all the API routes are defined.
if os.path.isdir("dist"):
    app.mount("/", StaticFiles(directory="dist", html=True), name="frontend")
    print("âœ… Static frontâ€‘end mounted from ./dist")
else:
    print("â„¹ï¸ dist/ directory not found at startup â€” frontend static mount skipped.")